# [[file:setup.pdf]]
* Setting up a new InterLex
If you want to set up a new InterLex from scratch the easiest way is to
use the =tgbugs/musl:interlex= docker image defined at
[[https://github.com/tgbugs/dockerfiles/blob/master/source.org#interlex]]
** Requirements
See the ebuild in tgbugs-overlay.
[[https://github.com/tgbugs/tgbugs-overlay/blob/master/dev-python/interlex/interlex-9999.ebuild]]
*** Setup for =pguri= postgres module
**** Gentoo
A =dev-db/pguri-9999.ebuild= is available in [[https://github.com/tgbugs/tgbugs-overlay]]
#+begin_src bash
layman -a tgbugs-overlay
emerge pguri
#+end_src
**** Ubuntu
#+begin_src bash
sudo apt-get install build-essential pkg-config liburiparser-dev postgresql-server-dev-all
export PKG_CONFIG_PATH=/usr/lib/x86_64-linux-gnu/pkgconfig
git clone https://github.com/petere/pguri.git
cd pguri
make PG_CONFIG=/usr/bin/pg_config
sudo make PG_CONFIG=/usr/bin/pg_config install
#+end_src
** Configuration
See example configs in
[[https://github.com/tgbugs/dockerfiles/blob/master/source.org#interlex]]
** Set up the database
1. Set up postgres for your operating system.
2. From the working directory run =bin/interlex-dbsetup $PORT $DBNAME=
   NOTE: if =$PORT= or =$DBNAME= is not provided then auth-config
   values for =test-port= and =test-database= will be used.
3. The first time you run =interlex-dbsetup= you will need to set
   the passwords for =interlex-admin= and =interlex-user= and then
   run =interlex-dbsetup $PORT $DBNAME= again. To accomplish this
   run =su postgres= then =psql= then =\password interlex-admin=
   and finally =\password interlex-user=.
4. Add the passwords for =interlex-admin= and =interlex-user= to =~/.pgpass=.
# 5. Run =interlex dbsetup= to add an initial 'authenticated' user.
** Set up the message broker
1. Install =rabbitmq= and add it and =epmd= to default services.
2. See =interlex-mqsetup=

* Run tests
1. From the working directory run
   =python -m unittest test/test_constraints.py=
   (this is also run by =interlex-dbsetup=)
2. From the working directory run =pytest=

* Load content
** Dump mysql for load from dev or local
This simplifies development and reduces the load of queries on the production database.
Additional tables might be considered for dumping, but at the moment these are sufficient.

Assuming you have set up port forwarding over ssh to production on port 33060.
You will be asked to paste the production db password when this is run.

Watch out for mismatched mariadb versions. =dev-db/mariadb-10.6.14= seems to be ok,
=10.11.5= not so much. Worst case use =docker run --network=host --rm -i mariadb:10.6 mysqldump=.
#+begin_src bash
mysqldump \
--user=nif_eelg_secure \
--password \
--host=127.0.0.1 \
--port=33060 \
nif_eelg \
terms \
term_annotations \
term_comments \
term_communities \
term_curie_catalog \
term_existing_ids \
term_ontologies \
term_ontology_join \
term_relationships \
term_superclasses \
term_synonyms \
term_versions > "interlex-dump-$(date -Is).sql"
#+end_src

On your target database run the following
#+begin_src sql :eval never
CREATE DATABASE nif_eelg;
CREATE USER nif_eelg_secure;
SET PASSWORD FOR nif_eelg_secure = PASSWORD('lolpassword');
GRANT ALL PRIVILEGES ON nif_eelg.* TO nif_eelg_secure;
#+end_src

Assuming you have set up port forwarding over ssh to your development mysql instance on port 33067.
You will be asked to paste the root password you set when configuring mariadb when this is run.
Adjust the input path to match your system.
#+begin_src bash :eval never
mysql \
--user=root \
--password \
--host=127.0.0.1 \
--port=33067 \
--database=nif_eelg \
 < ~/files/interlex/interlex-dump-2024-*.sql
#+end_src

** Sync with mysql
At this point you should be able to synchronize the database with the existing mysql interlex installation.
*WARNING* There is a bug in the current loading process and the loaded records do not match those generated by
the [[./../interlex/alt.py][alt server]] via [[./../interlex/dump.py][MysqlExport]].

1. Make sure you create a =~/.mypass= file that conforms to the syntax of =~/.pgpass=
   i.e. each line should look like =server.url.org:port:dbname:user:password=.
2. If you do not have direct access to the mysql database servers you may need to
   set up ssh forwarding in which case you should add the hostname of your devbox
   to =config.dev_remote_hosts= and forward to port =33060= to make use of
   [[https://github.com/tgbugs/interlex/blob/b458901a9abd2e3e36cd102caaf8e5c321a0e874/interlex/core.py#L528][core.py]].
3. Inside the venv run =interlex sync=
4. Once you drop into the IPython embed shell run =self.load()= and the load should commence.
   NOTE: there is no user auth at the moment so the code pretends to be =tgbugs=.
** Start the uri server
*** For development
run =interlex uri= in the venv.
*** For production
run =interlex-uri= in the venv.
*WARNING:* if you run in this way you will not be able to use =embed= to debug and you will
get strange errors.
** Start the message broker
for example
#+begin_src bash
INTERLEX_DATABASE=__interlex_sync BROKER_URL=$(python -c 'from interlex.config import auth; print(auth.get("mq-broker-url"))') EPYTHON=pypy3 celery --app interlex.tasks worker
#+end_src

** Load ontologies
#+begin_src bash :noweb yes :results code
uris=(
<<&ont-uris>>
)
for uri in ${uris[@]}; do
echo pypy3 -m interlex.ingest tgbugs "${uri}" --commit --debug '&&';
done
#+end_src

#+begin_src bash :noweb yes :results code
uris=(
<<&ont-uris>>
)
for uri in ${uris[@]}; do
echo pypy3 -m interlex.cli post resource --local --group tgbugs "${uri}";
done
#+end_src

#+name: &ont-uris
#+begin_src bash
# basics
http://www.w3.org/1999/02/22-rdf-syntax-ns
http://www.w3.org/2000/01/rdf-schema
http://www.w3.org/2002/07/owl
http://www.w3.org/2004/02/skos/core
https://www.dublincore.org/specifications/dublin-core/dcmi-terms/dublin_core_elements.ttl
https://www.dublincore.org/specifications/dublin-core/dcmi-terms/dublin_core_terms.ttl
http://protege.stanford.edu/plugins/owl/dc/dublincore.owl

# obo core
http://purl.obolibrary.org/obo/bfo.owl
http://purl.obolibrary.org/obo/ro.owl
http://purl.obolibrary.org/obo/iao.owl
# TODO auto resolve import chain stuff
https://raw.githubusercontent.com/SciCrunch/NIF-Ontology/dev/ttl/nif_backend.ttl
https://raw.githubusercontent.com/SciCrunch/NIF-Ontology/dev/ttl/BIRNLex-OBO-UBO.ttl
https://raw.githubusercontent.com/SciCrunch/NIF-Ontology/dev/ttl/BIRNLex_annotation_properties.ttl
https://raw.githubusercontent.com/SciCrunch/NIF-Ontology/dev/ttl/OBO_annotation_properties.ttl
http://www.geneontology.org/formats/oboInOwl

http://purl.obolibrary.org/obo/pato.owl

http://purl.obolibrary.org/obo/ncbitaxon/subsets/taxslim.owl

# core community ontologies
http://purl.obolibrary.org/obo/emapa.owl
http://purl.obolibrary.org/obo/cl.owl
http://purl.obolibrary.org/obo/hp.owl
http://purl.obolibrary.org/obo/mp.owl
http://purl.obolibrary.org/obo/upheno.owl
http://purl.obolibrary.org/obo/mondo.owl
http://purl.obolibrary.org/obo/doid.owl
http://purl.obolibrary.org/obo/go.owl
http://purl.obolibrary.org/obo/so.owl
http://purl.obolibrary.org/obo/uberon.owl

# large community ontologies
http://purl.org/sig/ont/fma.owl

http://ftp.ebi.ac.uk/pub/databases/chebi/ontology/chebi.owl.gz
# http://ftp.ebi.ac.uk/pub/databases/chebi/ontology/nightly/chebi.owl.gz
# insanely slow download on chebi
# http://purl.obolibrary.org/obo/chebi.owl
# https://lod.proconsortium.org/release.html

http://purl.obolibrary.org/obo/pr.owl

#http://ontology.neuinfo.org/NIF/ttl/external/pr.owl
#https://raw.githubusercontent.com/SciCrunch/NIF-Ontology/dev/ttl/NIF-GrossAnatomy.ttl
#+end_src

** Load curies
1. In the venv run
   src_bash[:eval never]{python -m interlex.cli post curies --local --group base}
   and then
   src_bash[:eval never]{python -m interlex.cli post curies --local --group ${YOURUSERNAME}}

# XXX if this is not done then the rendering on the no file extension
# pages is quite weird because there are no curies at all ... should
# fail over to the url but doesn't
* Performance notes
** load
#+begin_src bash
INTERLEX_DATABASE=__interlex_sync; bin/interlex-dbsetup 5432 ${INTERLEX_DATABASE}
INTERLEX_DATABASE=__interlex_sync pypy3 -m interlex.cli sync
#+end_src
wow yeah 20k ends up being way faster 26 seconds vs 60 seconds
apparently there is a nasty performance bug in the sql parser
when there are lots of parameters ...
wow that 26 seconds was the usert, but turns out ... plain old insert also runs faster
will have to try with psql 16 due to claimed changes in bulk load performance darn, that
is in =COPY= not insert

in conclusion 20k batch running faster than 80k batch with much less memory usage
and yes, those things are probably related since allocating memory is almost always
a bottleneck
** read
On =orpheus= the primary bottleneck seems to be the number of gunicorn workers.
For total failures to respond in within 5 seconds when confronted 8 workers
set at 50hz full blast. What is very strange is that the same set of failures
shows up for every worker on output, so I think something is funky with how
errors are getting passed back out. A different set do fail when looking at the
printout. HyperThreading doesn't seem to help here. Load seems split evenly between
the guni workers and postgres. Failures seem to happen in bursts at higher guni worker
counts.

| workers | avg failure % | cpu % sat all cores | effective rate Hz |
|---------+---------------+---------------------+-------------------|
|       2 |            50 |                  25 |                10 |
|       4 |             4 |                  60 |                16 |
|       4 |             9 |                  60 |                15 |
|       5 |             5 |                  80 |                18 |
|       8 |           4.5 |                 100 |                19 |
|       8 |             4 |                 100 |              19.5 |

Checking the logs, the ~20 Hz over 8 workers is indeed translating to about
160 requests per second, which still seems really low I should be able to generate
way more requests than 20/worker.

url_blaster is a ... bad piece of code.

#+begin_src bash
for id in {0100000..0120000};
do echo -e $id;
done | xargs -P 50 -r -n 1 curl -s "http://localhost:8606/base/ilx_${id}" > /dev/null
#+end_src
  
hits nearly 800 rps of 404s and

#+begin_src bash
for id in {0100000..0101000};
do echo -e "http://localhost:8606/base/ilx_${id}";
done | xargs -L 1 -P 100 curl -s > /dev/null
#+end_src
  
hits 180 rps running guni and db on the same server with 8 workers
(when requesting from not the server)
hits 140 rps running guni and db on the same server with 4 workers
  
tornado seems pretty fast for 8 as well? who knows
  
measuring with =time= from both the server and a remote shows that
we are hitting between 100 and 140 rps

who knows, maybe a materialized memory view would help for some of this,
though somehow I think the issue is probably in the python
  
pypy3 with sync worker has roughly the same performance, gevent is monstrously slow
gthread is about 20 rps slower than sync (1s over 1k requests), sync can get up to
~150rps, don't forget the cold boot effect on the first run which adds a second to everything
eventlet is about ~12rps or so slower than sync
(all for 8 workers, 4 workers is ~25rps slower for sync, 6 workers for sync seems
to get fairly close to performance with 8 and the total cpu usage is fairly close as well)
tornado with 6 workers seems to push the limits and is a bit faster than sync at ~155rps
taking it to 8 shows a slowdown to ~145 rps 4 workers drops it to 133rps 5 hits 150rps
so it seems that tornado with 6 is about the best for pypy3
  
pypy3 clearly faster with tornado than anything running 3.6, bonus is that rdflib +will
be way faster too if we can get the memory leak during serialization worked out+ is now
way faster since fixing the "turns out that allocating hundreds of thousands of empty
lists just looks like a memory leak" bug. pypy3 is also about 4x faster when dumping nt
straight from the database, peaking at about 80MBps to disk on the same computer while
python3.6 hits ~20MBps.

most of the pypy3 numbers are tainted by the fact that they were tested from the server
remotely there seems to be some cycling in the cpu usage, not sure why, but tornado at 8
seems like the best setup, eventlet might be ok too, more systematic testing would be needed

turning --log-level to critical gives maybe an extra second over 1000 requests

tested bjoern but got issues with hung processes and there is still quite high cpu usage
best approach seems like it will be to cache things since the issue is likely that we
are hitting python code to retrieve mostly static content anyway
