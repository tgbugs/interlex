-- CONNECT TO interlex_test USER "interlex-admin";
-- see notes in new-schema.sql

CREATE sequence if NOT exists interlex_ids_seq;

CREATE TABLE interlex_ids(
       id char(7) PRIMARY key DEFAULT LPAD(NEXTVAL('interlex_ids_seq')::text, 7, '0')
);

CREATE TABLE existing_iris(
       -- note that this table does NOT enumerate any uri.interlex.org identifiers
       -- the default/curated user will be the fail over
       -- do we need exclude rules? latest + original user will always be inserted
       -- but do we really even need latest to be explicit here?
       ilx_id char(7) NOT NULL,
       iri uri NOT NULL CHECK (uri_host(iri) NOT LIKE '%interlex.org'),
       group_id integer NOT NULL,
       CONSTRAINT fk__existing_iris__ilx_id__interlex_ids FOREIGN key (ilx_id) REFERENCES interlex_ids (id) match simple,
       CONSTRAINT fk__existing_iris__group_id__group FOREIGN key (group_id) REFERENCES groups (id) match simple,
       CONSTRAINT pk__existing_iris PRIMARY KEY (iri, group_id)
);

CREATE TYPE source_process AS ENUM ('FileFromIRI',  -- transitive closure be implemented using these on each file
                                    'FileFromPOST', -- we do not allow untrackable uploads use /<user>/ontologies
                                    'FileFromVCS',  -- this requires InterLex to clone the repo... which is ok I guess requires admin
                                    'NonOntologyFileFromVCS',

                                    'InterLexFile', -- source process for the collection of rules used to create a file
                                    -- these are the static sets of rules that are used to generate a file from a set of qualifiers

                                    'ReasonerOnFile', -- always reason on a 'file' as the abstraction never on subset directly
                                    -- when reasoning on an interlex defined file have to be careful to exclude already reasoned
                                    -- reasoned on file entries should have the file qualifier 
                                    -- 'ReasonerOnSubset',
                                    'InterLex'
                                    );

CREATE TABLE sources(
       -- aka files NOT ontologies
       -- sources do not tell you whether they are loading to or from, they are independent of that
       -- they are the unresolved graph subset
       id integer GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
       owner_group_id integer NOT NULL,
       interlex_source_path text NOT NULL, -- this is user + ontpath
       external_source_iri uri UNIQUE,
       -- CONSTRAINT pk__sources PRIMARY KEY (owner_group_id, interlex_source_path),
       CONSTRAINT un__sources UNIQUE (owner_group_id, interlex_source_path),
       CONSTRAINT fk__sources__owner_group_id__groups FOREIGN key (owner_group_id) REFERENCES groups (id) match simple
);

INSERT INTO sources (id, owner_group_id, interlex_source_path, external_source_iri) VALUES
       (0, 0, '/interlex.ttl', 'https://uri.interlex.org/base/interlex.ttl');  -- FIXME we really need an agnostic suffix :/ owl is too xml

CREATE FUNCTION create_user_source() RETURNS trigger AS $$
       BEGIN
           INSERT INTO sources (owner_group_id, interlex_source_path) VALUES
                  (NEW.id, '/interlex.ttl'); -- NOTE this is /<user>/interlex.ttl neet to be clear that this is not an ont path
           RETURN NULL;
       END;
$$ language plpgsql;

CREATE TRIGGER create_user_source AFTER INSERT ON users FOR EACH ROW EXECUTE PROCEDURE create_user_source();

CREATE TABLE source_triples(
       source_triples_hash bytea PRIMARY KEY,
       triples_count integer NOT NULL,
       source_id integer NOT NULL,
       CONSTRAINT fk__source_triples__source_id__sources FOREIGN key (source_id) REFERENCES sources (id) match simple
);
INSERT INTO source_triples VALUES (E'\\x00', 0, 0);

CREATE TABLE source_serialization(
       -- prov
       source_serialization_hash bytea PRIMARY KEY,
       source_triples_hash bytea NOT NULL,
       CONSTRAINT fk__source_ser__source_triples_hash__source_triples FOREIGN key (source_triples_hash)
                  REFERENCES source_triples (source_triples_hash) match simple
       -- group_id integer NOT NULL,
);
INSERT INTO source_serialization VALUES (E'\\x00', E'\\x00');

CREATE TYPE transform_rule AS enum ('EquivClassIntersection', 'EquivClassUnion', 'RestrictionSome', 'RestrictionAll', 'List');

CREATE TABLE load_processes(
       -- this is more for prov curiosity than real use right now
       id integer GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
       source_serialization_hash bytea NOT NULL,
       process_type source_process NOT NULL,
       source_iri uri,
       vcs_commit_ident text,
       datetime timestamp DEFAULT CURRENT_TIMESTAMP,
       group_id integer NOT NULL,
       user_id integer NOT NULL,
       CONSTRAINT fk__load_proc__source_ser__source_ser_hash__source_ser
                  FOREIGN key (source_serialization_hash)
                  REFERENCES source_serialization (source_serialization_hash) match simple
       -- TODO more forieng keys here
);

CREATE TABLE qualifiers(
       -- ordering of source_triples_hash for a given source id in time by group
       -- there are some use cases where dissociation from temporal order may be useful
       id integer GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
       source_id integer NOT NULL,  -- can get this from the load_process_id, but may be tricky to validate previous_q_id...
       source_triples_hash bytea, -- NOT NULL,
       -- group_id integer NOT NULL, -- redundant
       datetime timestamp DEFAULT CURRENT_TIMESTAMP, -- this also here for speed to avoid dealing with joins? or we leave date out of lopr?
       previous_qualifier_id integer NOT NULL CHECK (previous_qualifier_id <= id),  -- do we even need this anymore? no?

       equivalent_qualifier_id integer,  -- useful for exact duplicate loads by different users and quick rollbacks

       -- basically load process id + time, load processes are not sequential, but are treated as time invariant
       -- this allows us to do REALLY fast rollbacks by simply adding an equivalent qulaifier id to the old version
       -- and then setting previous qualifier as usual
       -- source_triples_hash could go in for completeness
       load_process_id integer CHECK (load_process_id IS NOT NULL OR (load_process_id IS NULL AND equivalent_qualifier_id IS NOT NULL)),
       source_serialization_hash bytea NOT NULL,
       -- useful for just completely ignoring a set of changes and starting back from the past in terms of content
       -- TODO need a check on previous qualifier_id to make sure its source_process_id matches
       -- but that is a super advanced feature

       CONSTRAINT fk__qualifiers__source_id__source_processes FOREIGN key (source_id) REFERENCES sources (id) match simple,
       CONSTRAINT fk__qualifiers__source_serialization_hash__source_serialization
                  FOREIGN key (source_serialization_hash) REFERENCES source_serialization (source_serialization_hash) match simple,
       CONSTRAINT fk__qualifiers__load_process_id__load_processes FOREIGN key (load_process_id) REFERENCES load_processes (id) match simple,
       -- CONSTRAINT fk__qualifiers__source_qualifier__qualifiers FOREIGN key (source_qualifier_id)
                  -- REFERENCES qualifiers (id) match simple,
       CONSTRAINT fk__qualifiers__previous_qualifier__qualifiers FOREIGN key (previous_qualifier_id)
                  REFERENCES qualifiers (id) match simple
);

CREATE TABLE qualifiers_current(
       source_id integer PRIMARY KEY,
       id integer NOT NULL,
       previous_ids integer[] NOT NULL,  -- no FK here, 'enforced' via population via trigger
       -- TODO CHECK qualifiers previous_qualifier_id = OLD.id aka previous_ids head? in trigger?
       CONSTRAINT fk__qualifiers__source_id__source_processes FOREIGN key (source_id) REFERENCES sources (id) match simple,
       CONSTRAINT fk__qualifiers_current__id__qualifiers FOREIGN key (id) REFERENCES qualifiers (id) match simple
);

CREATE FUNCTION qualifiers_to_current() RETURNS trigger AS $$
       BEGIN
           IF NOT EXISTS (SELECT * FROM qualifiers_current AS qc WHERE qc.source_id = NEW.source_id) THEN
              -- FIXME actually retrieve previous_qualifier_id
              INSERT INTO qualifiers_current (source_id, id, previous_ids) VALUES (NEW.source_id, NEW.id, '{0}');
           ELSE
              UPDATE qualifiers_current AS qc SET qc.id = NEW.id WHERE qc.source_id = NEW.source_id;
           END IF;
           RETURN NULL;
       END;
$$ language plpgsql;

CREATE FUNCTION qualifiers_current_array() RETURNS trigger AS $$
       BEGIN
           UPDATE qualifiers_current as qc
                  SET previous_ids = (NEW.source_id || NEW.previous_ids)
                  WHERE qc.source_id = NEW.source_id;
           -- TODO does NEW work for this and restrict to row automatically?
           RETURN NULL;
       END;
$$ language plpgsql;

CREATE TRIGGER qualifiers_to_current AFTER INSERT OR UPDATE ON qualifiers FOR EACH ROW EXECUTE PROCEDURE qualifiers_to_current();
CREATE TRIGGER qualifiers_current_array AFTER INSERT ON qualifiers_current
       FOR EACH ROW EXECUTE PROCEDURE qualifiers_current_array();
CREATE TRIGGER qualifiers_current_array_id_only AFTER UPDATE ON qualifiers_current
       FOR EACH ROW WHEN (OLD.id IS DISTINCT FROM NEW.id) EXECUTE PROCEDURE qualifiers_current_array();

CREATE INDEX qualifiers_id_index ON qualifiers (id);

-- the root qualifier the root for all new source process qualifiers

CREATE FUNCTION create_source_qualifier() RETURNS trigger AS $$
       -- creates the root for all source process qualifiers, is equivalent to itself since there is no load id
       -- TODO create group interlex source process -> create source_process qualifier
       -- DECLARE
           -- prev_qual integer;
       BEGIN
           --IF NOT EXISTS (SELECT * FROM qualifiers AS q WHERE q.source_process_id = NEW.id)
           --THEN
                -- TODO figure out the proper group to derive previous from maybe latest? or is it curated?
                -- SELECT id INTO STRICT prev_qual FROM qualifiers_current as q WHERE q.group_id = 1 AND;
                -- load processes are where we will need to actually look up the previous qualifier id
           INSERT INTO qualifiers (source_id, source_triples_hash, group_id, equivalent_qualifier_id, previous_qualifier_id)
           source_id, source_triples_hash, group_id, -- datetime
           previous_qualifier_id, equivalent_qualifier_id, load_process_id, source_serialization_hash
           VALUES (NEW.id, NEW.id, 0);
           --END IF;
           RETURN NULL;
       END;
$$ language plpgsql;

CREATE TRIGGER create_source_qualifier AFTER INSERT ON sources FOR EACH ROW EXECUTE PROCEDURE create_source_qualifier();

CREATE FUNCTION create_load_process_qualifier() RETURNS trigger AS $$  -- TODO should be source_triples_hash if anything
       DECLARE
           prev_qual_id integer;
       BEGIN
           -- boy... this seems a lot slower than the array version
           SELECT id INTO STRICT prev_qual_id FROM qualifiers_current AS qc WHERE qc.source_process_id = NEW.source_process_id;
           INSERT INTO qualifiers (source_process_id, load_procedss_id, previous_qualifier_id)
                  VALUES (NEW.source_process_id, NEW.id, prev_qual_id);
           RETURN NULL;
       END;
$$ language plpgsql;

CREATE TABLE predicate_cardinality(
       p uri PRIMARY KEY,  -- only include predicates with actual limites
       cardinality integer NOT NULL
       -- rank integer NOT NULL DEFAULT -1 -- want or use the ranking in pyontutils??
);

CREATE TYPE rdf_type AS enum ('Class', 'Ontology', 'AnnotationProperty', 'ObjectProperty', 'DataPropery');  -- TODO

CREATE TABLE subject_types(
       s uri NOT NULL,
       o rdf_type NOT NULL,
       CONSTRAINT pk__types__s_o PRIMARY KEY (s, o)
);

CREATE TABLE triples(
       id integer GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, -- preferred for 10+
       s uri,
       s_blank integer, -- internal bnode counter for isomorphism checks
       p uri NOT NULL,
       o uri,
       o_lit text,
       o_blank integer, -- this is internal for (s_blank p o_blank) and triples.id for (s, p, o_blank)
       datatype uri CHECK (o_lit IS NULL OR o_lit IS NOT NULL AND datatype IS NOT NULL),
       language varchar(10),
       subgraph_hash bytea CHECK (s_blank IS NULL OR s_blank IS NOT NULL AND subgraph_hash IS NOT NULL),
       CHECK ((s IS NOT NULL AND s_blank IS NULL) OR
              (s IS NULL AND s_blank IS NOT NULL)),
       CHECK ((o IS NOT NULL AND o_lit IS NULL AND o_blank IS NULL) OR
              (o IS NULL AND o_lit IS NOT NULL AND o_blank IS NULL) OR
              (o IS NULL AND o_lit IS NULL AND o_blank IS NOT NULL)),
       CHECK (o_blank <> s_blank),
       CONSTRAINT un__triples__s_p_o UNIQUE (s, p, o),
       CONSTRAINT un__triples__s_p_o_lit UNIQUE (s, p, o_lit, datatype, language),
       CONSTRAINT un__triples__s_p_o_blank UNIQUE (s, p, o_blank)
);

/*
(9997 null 0 rdf:type           null null owl:Restriction null null ASDF87SDF7A6SD75A5)
(9998 null 0 owl:onProperty     null null BFO:0000050     null null ASDF87SDF7A6SD75A5)
(9999 null 0 owl:someValuesFrom null null UBERON:0000955  null null ASDF87SDF7A6SD75A5)

(?? null ?? owl:onProperty null null BFO:0000050 null null ASDF87SDF7A6SD75A5)
(?? null ?? owl:onProperty null null BFO:0000050 null null ASDF87SDF7A6SD75A5)
(?? null ?? owl:onProperty null null BFO:0000050 null null ASDF87SDF7A6SD75A5)
(?? null ?? owl:onProperty null null BFO:0000050 null null ASDF87SDF7A6SD75A5)
*/

-- what about (?? null ?? owl:onProperty null null BFO:0000050 null null ???)
-- we cannot reuse subgraph triples, we can reuse entire subgraphs starting from (s, p, o_blank)

/*
CREATE sequence if NOT exists triples_all_id_seq;

CREATE TABLE triples_uri(
       -- lifted by default
       id integer PRIMARY KEY DEFAULT nextval('triples_all_id_seq'),
       s uri NOT NULL,
       p uri NOT NULL,
       o uri NOT NULL,
       CONSTRAINT un__triples_uri__s_p_o UNIQUE (s, p, o)
       -- CONSTRAINT pk__triples_uri PRIMARY key (s, p, o, qualifier_id),  -- TODO need transform id too?
       -- CONSTRAINT fk__triples_uri__qualifier_id__qualifiers FOREIGN key (qualifier_id) REFERENCES qualifiers (id) match simple
);

CREATE TABLE triples_literal(
       -- lifted by default
       id integer PRIMARY KEY DEFAULT nextval('triples_all_id_seq'),
       s uri NOT NULL,
       p uri NOT NULL,
       o text NOT NULL,
       datatype uri NOT NULL,
       language varchar(10),
       CONSTRAINT un__triples_literal__s_p_o UNIQUE (s, p, o)  -- TODO need transform id too?
       -- CONSTRAINT fk__triples_literal__qualifier_id__qualifiers FOREIGN key (qualifier_id) REFERENCES qualifiers (id) match simple
);

CREATE TABLE blank_to_subgraph(
       id integer PRIMARY KEY DEFAULT nextval('triples_all_id_seq'),
       subgraph_hash bytea UNIQUE NOT NULL
);

CREATE TABLE triples_subgraph(
       -- NOTE this is how we store unlifted, the lifted store will look different
       -- TODO seriously consider using arrays for lists? well, these fellows are unlifted
       subgraph_hash bytea NOT NULL, -- TODO need deterministic way to do this (woo ttl)
       s integer NOT NULL,  -- start bnodes as zero for each subgraph hash
       p uri NOT NULL,
       o uri,
       o_lit text,
       o_blank integer,
       datatype uri CHECK (o_lit IS NULL OR o_lit IS NOT NULL AND datatype IS NOT NULL),
       language varchar(10),
       CHECK ((o IS NOT NULL AND o_lit IS NULL AND o_blank IS NULL) OR
              (o IS NULL AND o_lit IS NOT NULL AND o_blank IS NULL) OR
              (o IS NULL AND o_lit IS NULL AND o_blank IS NOT NULL)),
       -- CONSTRAINT ch__triples_subgraph__some_o CHECK
                  -- ((o_uri NOT NULL AND o_lit IS NULL AND o_blank IS NULL) OR
                   -- (o_uri IS NULL AND o_lit NOT NULL AND o_blank IS NULL) OR
                   -- (o_uri IS NULL AND o_lit IS NULL AND o_blank NOT NULL)),
       CONSTRAINT pk__triples_subgraph PRIMARY KEY (subgraph_hash, s),
       CONSTRAINT un__triples_subgraph__sh_s_p_o UNIQUE (subgraph_hash, s, p, o, o_lit, o_blank, datatype, language)
       -- CONSTRAINT triples_subgraph__subgraph_hash__fk__blank_to_subgraph FOREIGN KEY (subgraph_hash) REFERENCES blank_to_subgraph (subgraph_hash) match simple
       -- CONSTRAINT fk__triples_subgraph__subgraph_hash__triples FOREIGN KEY (subgraph_hash) REFERENCES triples (o_blank) match simple
       -- FIXME not clear if there is a way to enable this fk...
);

CREATE FUNCTION insert_subgraph() RETURNS trigger AS $$
       BEGIN
           INSERT INTO blank_to_subgraph (subgraph_hash) VALUES (NEW.subgraph_hash);
           -- FIXME probably will break due to FOR EACH STATEMENT
           -- TODO I have no idea what the right way is to do this
           -- when I have a complete subgraph with random bnodes
           RETURN NULL;
       END;
$$ language plpgsql;

CREATE TRIGGER insert_subgraph AFTER INSERT ON triples_subgraph FOR EACH STATEMENT EXECUTE PROCEDURE insert_subgraph();

CREATE TABLE triples_blank(
       id integer PRIMARY KEY DEFAULT nextval('triples_all_id_seq'),
       s uri NOT NULL,
       p uri NOT NULL,
       blank_id integer NOT NULL,
       CONSTRAINT fk__triples_blank__blank_id__qualifiers FOREIGN key (blank_id) REFERENCES blank_to_subgraph (id) match simple
);

*/


/* -- these are all handled using the qualifiers on the source

CREATE TABLE triples_lifted_uri(
       -- lifted qualifier?
       id integer NOT NULL,
       s uri NOT NULL,
       p uri NOT NULL,
       o uri NOT NULL,
       CONSTRAINT fk__triples_lifted_uri__id__triples_subgraph FOREIGN key (id) REFERENCES triples_blank (id) match simple
);


CREATE TABLE triples_lifted_literal(
       -- FIXME not clear we need this?
       id integer NOT NULL,
       s uri NOT NULL,
       p uri NOT NULL,
       o text NOT NULL,
       CONSTRAINT fk__triples_lifted_literal__id__triples_subgraph FOREIGN key (id) REFERENCES triples_blank (id) match simple
);

CREATE TABLE triples_lifted_complex(
       id integer NOT NULL,
       type transform_rule NOT NULL,
       CONSTRAINT fk__triples_lifted_complex__id__triples_subgraph FOREIGN key (id) REFERENCES triples_blank (id) match simple
       -- there are 2 ways this could be done, either as uri, uri, blank -> triples_blank or blank, *, * -> blank_to_subgraph
       -- I think that uri, uri, blank is a better place to anchor even though the transform rule might switch out the predicate
);
*/

CREATE TABLE triple_qualifiers(
       triple_id integer NOT NULL,
       qualifier_id integer NOT NULL,
       CONSTRAINT fk__triple_qualifiers__qualifier_id__qualifiers FOREIGN key (qualifier_id) REFERENCES qualifiers (id) match simple
);

CREATE TABLE deletions(
       triple_id integer NOT NULL,
       qualifier_id integer NOT NULL,  -- gone here
       previous_qualifier_id integer NOT NULL CHECK (qualifier_id > previous_qualifier_id),  -- present here
       -- how to handle cases of open world where user never added triples themselves? zap at origin?
       -- have to zap based on the user's ranking at that point in time
       -- if you change the ranking then you can change the meaning of qualifiers
       CONSTRAINT fk__triple_qualifiers__qualifier_id__qualifiers FOREIGN key (qualifier_id) REFERENCES qualifiers (id) match simple,
       CONSTRAINT fk__triple_qualifiers__previous_qualifier_id__qualifiers FOREIGN key (qualifier_id) REFERENCES qualifiers (id) match simple
);

CREATE TABLE annotations(
       triple_id integer NOT NULL,  -- ah, and now we see the problem with having 3 tables
       annotation_triple_id integer NOT NULL,
       fk__
);
