-- CONNECT TO interlex_test USER "interlex-admin";
-- see notes in new-schema.sql

CREATE sequence if NOT exists interlex_ids_seq;

CREATE TABLE interlex_ids(
       id char(7) PRIMARY key DEFAULT LPAD(NEXTVAL('interlex_ids_seq')::text, 7, '0')
);

CREATE TABLE existing_iris(
       -- note that this table does NOT enumerate any uri.interlex.org identifiers
       -- the default/curated user will be the fail over
       -- do we need exclude rules? latest + original user will always be inserted
       -- but do we really even need latest to be explicit here?
       ilx_id char(7) NOT NULL,
       iri uri NOT NULL CHECK (uri_host(iri) NOT LIKE '%interlex.org'),
       group_id integer NOT NULL,
       CONSTRAINT fk__existing_iris__ilx_id__interlex_ids FOREIGN key (ilx_id) REFERENCES interlex_ids (id) match simple,
       CONSTRAINT fk__existing_iris__group_id__group FOREIGN key (group_id) REFERENCES groups (id) match simple,
       CONSTRAINT pk__existing_iris PRIMARY KEY (iri, group_id)
);

CREATE TYPE source_process AS ENUM ('FileFromIRI',  -- transitive closure be implemented using these on each file
                                    'FileFromPOST', -- we do not allow untrackable uploads use /<user>/ontologies
                                    'FileFromVCS',  -- this requires InterLex to clone the repo... which is ok I guess requires admin
                                    'NonOntologyFileFromVCS',

                                    'InterLexFile', -- source process for the collection of rules used to create a file
                                    -- these are the static sets of rules that are used to generate a file from a set of qualifiers

                                    'ReasonerOnFile', -- always reason on a 'file' as the abstraction never on subset directly
                                    -- when reasoning on an interlex defined file have to be careful to exclude already reasoned
                                    -- reasoned on file entries should have the file qualifier 
                                    -- 'ReasonerOnSubset',
                                    'InterLex'
                                    );
CREATE TABLE source_processes(
       id integer GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
       process_type source_process NOT NULL,
       group_id integer NOT NULL,
       -- TODO check on group_id is a user_id if process type is 'InterLex'
       -- groups may run the reasoner and create ontology files along with the rules, the users who created the rules will be tracked
       source_iri uri,  -- file://tmp/vcs-load/<user>/NIF-Ontology/ttl/nif.ttl for local VCS? needs to be invariant
       ontology_iri uri,  -- POST https://uri.interlex.org/<user>/ontologies/<path:ont_path>  -- MUCH easier than other ways for upoads
       CHECK (process_type >= 'ReasonerOnFile' OR source_iri IS NOT NULL),
       CHECK (process_type >= 'ReasonerOnFile' OR ontology_iri IS NOT NULL),  -- TODO not quite right
       CONSTRAINT un__source_processes UNIQUE (group_id, ontology_iri)
);
INSERT INTO source_processes VALUES (0, 'InterLex', 0, NULL, NULL);  -- the base source process won't trigger since not an org
-- CREATE TRIGGER create_org_source_process AFTER INSERT ON orgs FOR EACH ROW EXECUTE PROCEDURE create_group_source_process();
-- it is not possible to edit interlex as an org

/*
-- Not clear we need/should do this with a trigger, there is too much missing information
CREATE FUNCTION create_ontology_source_process(OUT id integer) RETURNS trigger AS $$
       BEGIN
           INSERT INTO source_processes (process_type, group_id, source_iri, ontology_iri) VALUES ('InterLex', NEW.id) RETURNING id;
       END;
$$ language plpgsql;

CREATE TRIGGER create_ontology_source_process AFTER INSERT ON ontologies FOR EACH ROW EXECUTE PROCEDURE create_ontology_source_process();
*/

CREATE TABLE old_sources(
       -- TODO replace with just ontologies (ie ontology files)
       -- subgraphs ...
       -- ontology file or user edit
       id integer GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
       ontology_iri uri UNIQUE,  -- POST https://uri.interlex.org/<user>/ontologies/<path:ont_path>  -- MUCH easier than other ways for upoads
       owner_user_id integer NOT NULL,
       user_id integer,  -- maybe /<user>/ontologies/interlex.ttl to make it clearer?, users may have other users as sources
       ontology_path text not null,  -- the local path to either local or remote this gets into 'ownership' 1:1ness for users to external
       -- if your source doesn't have an iri that we can track, go get one from /ontologies/
       CONSTRAINT fk__sources__user_id__users FOREIGN key (user_id) REFERENCES users (id) match simple,
       CHECK ((user_id IS NOT NULL AND ontology_iri IS NULL) OR (user_id IS NULL AND ontology_iri IS NOT NULL))
);

CREATE TABLE sources(
       -- aka files NOT ontologies
       -- sources do not tell you whether they are loading to or from, they are independent of that
       -- they are the unresolved graph subset
       id integer GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
       owner_group_id integer NOT NULL
       interlex_source_path text NOT NULL, -- this is user + ontpath
       external_source_iri uri UNIQUE,
       -- CONSTRAINT pk__sources PRIMARY KEY (owner_group_id, interlex_source_path),
       CONSTRAINT un__sources UNIQUE (owner_group_id, interlex_source_path),
       CONSTRAINT fk__sources__owner_group_id__groups FOREIGN key (owner_group_id) REFERENCES groups (id) match simple
);

CREATE FUNCTION create_user_source() RETURNS trigger AS $$
       BEGIN
           INSERT INTO sources (owner_group_id, interlex_source) VALUES
                  (NEW.id, '/interlex.ttl'); -- NOTE this is /<user>/interlex.ttl neet to be clear that this is not an ont path
           RETURN NULL;
       END;
$$ language plpgsql;

CREATE TRIGGER create_user_source AFTER INSERT ON users FOR EACH ROW EXECUTE PROCEDURE create_user_source();

CREATE TABLE source_triples(
       source_triples_hash bytea PRIMARY KEY,
       triples_count integer NOT NULL,
       source_id integer NOT NULL,
       CONSTRAINT fk__source_triples__source_id__sources FOREIGN key (source_id) REFERENCES sources (id) match simple
);

CREATE TABLE source_serialization(
       -- prov
       source_serialization_hash bytea PRIMARY KEY,
       source_triples_hash bytea NOT NULL,
       CONSTRAINT fk__source_serialization__source_triples_hash__source_triples FOREIGN key (source_triples_hash)
                  REFERENCES sources_triples (source_triples_hash) match simple
       -- group_id integer NOT NULL,
);

CREATE TABLE load_processes(
       -- this is more for prov curiosity than real use right now
       id integer 
       source_hash bytea NOT NULL,
       process_type source_process NOT NULL,
       source_iri uri,
       vcs_commit_ident text,
       datetime timestamp DEFAULT CURRENT_TIMESTAMP,
       group_id integer NOT NULL,
       CONSTRAINT fk__load_processes__source_serialization__source_serialization_hash__source_serialization
                  FOREIGN key (source_serialization_hash)
                  REFERENCES sources_triples (source_triples_hash) match simple
);

CREATE TABLE old_load_processes(
       -- id integer GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
       source_id integer NOT NULL,
       --source_process_id integer NOT NULL,
       -- ontology_version_iri -- ??
       -- TODO source_hash vs source_triples_hash so invariant between owl and ttl? that's why ontology_iri and source_iri differ
       -- but also, for equivalent_qualifier_id we will have to do the equivalent of source_triples_hash anyway...
       -- so probably is worth storing
       -- source_hash varchar(64) NOT NULL CHECK (length(source_hash) >= 40), -- TODO figure out the right datatype, 64 covers SHA256 can enforce > 40, also bytea or something else to save these properly
       source_hash bytea UNIQUE NOT NULL, -- TODO len check?
       triples_count integer NOT NULL,
       source_triples_hash bytea NOT NULL, -- TODO len check?
       -- source_triples_hash varchar(64) NOT NULL CHECK (length(source_hash) >= 40), -- not unique but is the last step for every load
                                       -- this allows us to identify exact deletions and readditions quickly and easily
                                       -- eg under file renaming, this does mean we probably want to differentiate between
                                       -- ontology content and non-ontology content, like the ontology header section
                                       -- as a known subset that could be invariant? subgraph isomorphism is NP complete
                                       -- but checking to see whether all members of a load are the subset of some other
                                       -- single single qualified load is actually fairly easy because you only need to
                                       -- find a single counter example where the new load has an edge that load does not
                                       -- though obviously the worstcase runtime for that algorithem is awful...
                                       -- total triple count per load also helps here
       vcs_commit_ident varchar(64), -- used to track the vcs load id, not just git hashes hg ids, and svn revision ids etc
       -- CONSTRAINT un__load_processes__source_process_id__source_hash UNIQUE (source_process_id, source_hash)
       CONSTRAINT un__load_processes__source_process_id__source_hash UNIQUE (source_id, source_hash)
       -- we very much hope that source_hash is also unique...
);

-- CREATE TRIGGER new_load_process AFTER INSERT ON ontologies -- don't think we need this, the application will create these

CREATE TABLE qualifiers(
       -- basically load process id + time, load processes are not sequential, but are treated as time invariant
       -- this allows us to do REALLY fast rollbacks by simply adding an equivalent qulaifier id to the old version
       -- and then setting previous qualifier as usual
       id integer GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
       equivalent_qualifier_id integer,  -- useful for exact duplicate loads by different users and quick rollbacks
       source_process_id integer NOT NULL,  -- can get this from the load_process_id, but may be tricky to validate previous_q_id...
       load_process_id integer CHECK (load_process_id IS NOT NULL OR (load_process_id IS NULL AND equivalent_qualifier_id IS NOT NULL)),
       datetime timestamp DEFAULT CURRENT_TIMESTAMP, -- this also here for speed to avoid dealing with joins? or we leave date out of lopr?
       previous_qualifier_id integer NOT NULL CHECK (previous_qualifier_id <= id),  -- do we even need this anymore? no?
       -- useful for just completely ignoring a set of changes and starting back from the past in terms of content
       -- TODO need a check on previous qualifier_id to make sure its source_process_id matches
       -- but that is a super advanced feature
       CONSTRAINT fk__qualifiers__source_process_id__source_processes FOREIGN key (source_process_id) REFERENCES source_processes (id) match simple,
       CONSTRAINT fk__qualifiers__load_process_id__load_processes FOREIGN key (load_process_id) REFERENCES load_processes (id) match simple,
       -- CONSTRAINT fk__qualifiers__source_qualifier__qualifiers FOREIGN key (source_qualifier_id)
                  -- REFERENCES qualifiers (id) match simple,
       CONSTRAINT fk__qualifiers__previous_qualifier__qualifiers FOREIGN key (previous_qualifier_id)
                  REFERENCES qualifiers (id) match simple
);

CREATE TABLE qualifiers_current(
       source_id integer PRIMARY KEY,
       id integer NOT NULL,
       previous_ids integer[] NOT NULL,  -- no FK here, 'enforced' via population via trigger
       -- TODO CHECK qualifiers previous_qualifier_id = OLD.id aka previous_ids head? in trigger?
       CONSTRAINT fk__qualifiers__source_process_id__source_processes FOREIGN key (source_process_id) REFERENCES source_processes (id) match simple,
       CONSTRAINT fk__qualifiers_current__id__qualifiers FOREIGN key (id) REFERENCES qualifiers (id) match simple
);

CREATE FUNCTION qualifiers_to_current() RETURNS trigger AS $$
       BEGIN
           IF NOT EXISTS (SELECT * FROM qualifiers_current AS qc WHERE qc.source_process_id = NEW.source_process_id) THEN
              -- FIXME actually retrieve previous_qualifier_id
              INSERT INTO qualifiers_current (source_process_id, id, previous_ids) VALUES (NEW.source_process_id, NEW.id, '{0}');
           ELSE
              UPDATE qualifiers_current AS qc SET qc.id = NEW.id WHERE qc.source_process_id = NEW.source_process_id;
           END IF;
           RETURN NULL;
       END;
$$ language plpgsql;

CREATE FUNCTION qualifiers_current_array() RETURNS trigger AS $$
       BEGIN
           UPDATE qualifiers_current as qc
                  SET previous_ids = (NEW.source_process_id || NEW.previous_ids)
                  WHERE qc.source_process_id = NEW.source_process_id;
           -- TODO does NEW work for this and restrict to row automatically?
           RETURN NULL;
       END;
$$ language plpgsql;

CREATE TRIGGER qualifiers_to_current AFTER INSERT OR UPDATE ON qualifiers FOR EACH ROW EXECUTE PROCEDURE qualifiers_to_current();
CREATE TRIGGER qualifiers_current_array AFTER INSERT ON qualifiers_current
       FOR EACH ROW EXECUTE PROCEDURE qualifiers_current_array();
CREATE TRIGGER qualifiers_current_array_id_only AFTER UPDATE ON qualifiers_current
       FOR EACH ROW WHEN (OLD.id IS DISTINCT FROM NEW.id) EXECUTE PROCEDURE qualifiers_current_array();

CREATE INDEX qualifiers_id_index ON qualifiers (id);
INSERT INTO qualifiers (id, source_process_id, equivalent_qualifier_id, previous_qualifier_id) VALUES (0, 0, 0, 0);
-- the root qualifier the root for all new source process qualifiers

CREATE FUNCTION create_source_process_qualifier() RETURNS trigger AS $$
       -- creates the root for all source process qualifiers, is equivalent to itself since there is no load id
       -- TODO create group interlex source process -> create source_process qualifier
       -- DECLARE
           -- prev_qual integer;
       BEGIN
           --IF NOT EXISTS (SELECT * FROM qualifiers AS q WHERE q.source_process_id = NEW.id)
           --THEN
                -- TODO figure out the proper group to derive previous from maybe latest? or is it curated?
                -- SELECT id INTO STRICT prev_qual FROM qualifiers_current as q WHERE q.group_id = 1 AND;
                -- load processes are where we will need to actually look up the previous qualifier id
           INSERT INTO qualifiers (source_process_id, equivalent_qualifier_id, previous_qualifier_id) VALUES (NEW.id, NEW.id, 0);
           --END IF;
           RETURN NULL;
       END;
$$ language plpgsql;

CREATE TRIGGER create_source_process_qualifier AFTER INSERT ON source_processes FOR EACH ROW EXECUTE PROCEDURE create_source_process_qualifier();

CREATE FUNCTION create_load_process_qualifier() RETURNS trigger AS $$
       DECLARE
           prev_qual_id integer;
       BEGIN
           -- boy... this seems a lot slower than the array version
           SELECT id INTO STRICT prev_qual_id FROM qualifiers_current AS qc WHERE qc.source_process_id = NEW.source_process_id;
           INSERT INTO qualifiers (source_process_id, load_procedss_id, previous_qualifier_id)
                  VALUES (NEW.source_process_id, NEW.id, prev_qual_id);
           RETURN NULL;
       END;
$$ language plpgsql;

CREATE TABLE predicate_cardinality(
       p uri PRIMARY KEY,  -- only include predicates with actual limites
       cardinality integer NOT NULL
       -- rank integer NOT NULL DEFAULT -1 -- want or use the ranking in pyontutils??
);

CREATE TABLE subject_types(
       s uri NOT NULL,
       o rdf_type NOT NULL,
       CONSTRAINT pk__types__s_o PRIMARY KEY (s, o)
);

CREATE TABLE triples(
       id integer GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, -- preferred for 10+
       s uri NOT NULL,
       p uri NOT NULL,
       o uri,
       o_lit text,
       o_blank bytea,
       datatype uri CHECK (o_lit IS NULL OR o_lit IS NOT NULL AND datatype IS NOT NULL),
       language varchar(10),
       CHECK ((o IS NOT NULL AND o_lit IS NULL AND o_blank IS NULL) OR
              (o IS NULL AND o_lit IS NOT NULL AND o_blank IS NULL) OR
              (o IS NULL AND o_lit IS NULL AND o_blank IS NOT NULL)),
       CONSTRAINT un__triples__s_p_o UNIQUE (s, p, o),
       CONSTRAINT un__triples__s_p_o_lit UNIQUE (s, p, o_lit, datatype, language),
       CONSTRAINT un__triples__s_p_o_blank UNIQUE (s, p, o_blank)
       CONSTRAINT fk__triples_uri__qualifier_id__qualifiers FOREIGN key (qualifier_id) REFERENCES qualifiers (id) match simple
);

CREATE sequence if NOT exists triples_all_id_seq;

CREATE TABLE triples_uri(
       -- lifted by default
       id integer PRIMARY KEY DEFAULT nextval('triples_all_id_seq'),
       s uri NOT NULL,
       p uri NOT NULL,
       o uri NOT NULL,
       CONSTRAINT un__triples_uri__s_p_o UNIQUE (s, p, o)
       -- CONSTRAINT pk__triples_uri PRIMARY key (s, p, o, qualifier_id),  -- TODO need transform id too?
       -- CONSTRAINT fk__triples_uri__qualifier_id__qualifiers FOREIGN key (qualifier_id) REFERENCES qualifiers (id) match simple
);

CREATE TABLE triples_literal(
       -- lifted by default
       id integer PRIMARY KEY DEFAULT nextval('triples_all_id_seq'),
       s uri NOT NULL,
       p uri NOT NULL,
       o text NOT NULL,
       datatype uri NOT NULL,
       language varchar(10),
       CONSTRAINT un__triples_literal__s_p_o UNIQUE (s, p, o)  -- TODO need transform id too?
       -- CONSTRAINT fk__triples_literal__qualifier_id__qualifiers FOREIGN key (qualifier_id) REFERENCES qualifiers (id) match simple
);

CREATE TABLE blank_to_subgraph(
       id integer PRIMARY KEY DEFAULT nextval('triples_all_id_seq'),
       subgraph_hash bytea UNIQUE NOT NULL
);

CREATE TABLE triples_subgraph(
       -- NOTE this is how we store unlifted, the lifted store will look different
       -- TODO seriously consider using arrays for lists? well, these fellows are unlifted
       subgraph_hash bytea NOT NULL, -- TODO need deterministic way to do this (woo ttl)
       s integer NOT NULL,  -- start bnodes as zero for each subgraph hash
       p uri NOT NULL,
       o uri,
       o_lit text,
       o_blank integer,
       datatype uri CHECK (o_lit IS NULL OR o_lit IS NOT NULL AND datatype IS NOT NULL),
       lang varchar(10),
       CHECK ((o IS NOT NULL AND o_lit IS NULL AND o_blank IS NULL) OR
              (o IS NULL AND o_lit IS NOT NULL AND o_blank IS NULL) OR
              (o IS NULL AND o_lit IS NULL AND o_blank IS NOT NULL)),
       -- CONSTRAINT ch__triples_subgraph__some_o CHECK
                  -- ((o_uri NOT NULL AND o_lit IS NULL AND o_blank IS NULL) OR
                   -- (o_uri IS NULL AND o_lit NOT NULL AND o_blank IS NULL) OR
                   -- (o_uri IS NULL AND o_lit IS NULL AND o_blank NOT NULL)),
       CONSTRAINT pk_triples_subgraph PRIMARY KEY (subgraph_hash, s),
       CONSTRAINT un__triples_subgraph__sh_s_p_o UNIQUE (subgraph_hash, s, p, o, o_lit, o_blank, datatype, language),
       -- CONSTRAINT triples_subgraph__subgraph_hash__fk__blank_to_subgraph FOREIGN KEY (subgraph_hash) REFERENCES blank_to_subgraph (subgraph_hash) match simple
       CONSTRAINT triples_subgraph__subgraph_hash__fk__triples FOREIGN KEY (o_blank) REFERENCES triples (o_blank) match simple
);

CREATE FUNCTION insert_subgraph() RETURNS trigger AS $$
       BEGIN
           INSERT INTO blank_to_subgraph (subgraph_hash) VALUES (NEW.subgraph_hash);
           -- FIXME probably will break due to FOR EACH STATEMENT
           -- TODO I have no idea what the right way is to do this
           -- when I have a complete subgraph with random bnodes
           RETURN NULL;
       END;
$$ language plpgsql;

CREATE TRIGGER insert_subgraph AFTER INSERT ON triples_subgraph FOR EACH STATEMENT EXECUTE PROCEDURE insert_subgraph();

CREATE TABLE triples_blank(
       id integer PRIMARY KEY DEFAULT nextval('triples_all_id_seq'),
       s uri NOT NULL,
       p uri NOT NULL,
       blank_id integer NOT NULL,
       CONSTRAINT fk__triples_blank__blank_id__qualifiers FOREIGN key (blank_id) REFERENCES blank_to_subgraph (id) match simple
);

CREATE TYPE transform_rule AS enum ('EquivClassIntersection', 'EquivClassUnion', 'RestrictionSome', 'RestrictionAll', 'List');

CREATE TABLE triples_lifted_uri(
       -- lifted qualifier?
       id integer NOT NULL,
       s uri NOT NULL,
       p uri NOT NULL,
       o uri NOT NULL,
       CONSTRAINT fk__triples_lifted_uri__id__triples_subgraph FOREIGN key (id) REFERENCES triples_blank (id) match simple
);

CREATE TABLE triples_lifted_literal(
       -- FIXME not clear we need this?
       id integer NOT NULL,
       s uri NOT NULL,
       p uri NOT NULL,
       o text NOT NULL,
       CONSTRAINT fk__triples_lifted_literal__id__triples_subgraph FOREIGN key (id) REFERENCES triples_blank (id) match simple
);

CREATE TABLE triples_lifted_complex(
       id integer NOT NULL,
       type transform_rule NOT NULL,
       CONSTRAINT fk__triples_lifted_complex__id__triples_subgraph FOREIGN key (id) REFERENCES triples_blank (id) match simple
       -- there are 2 ways this could be done, either as uri, uri, blank -> triples_blank or blank, *, * -> blank_to_subgraph
       -- I think that uri, uri, blank is a better place to anchor even though the transform rule might switch out the predicate
);

CREATE TABLE triple_qualifiers(
       triple_id integer NOT NULL,
       qualifier_id integer NOT NULL,
       CONSTRAINT fk__triple_qualifiers__qualifier_id__qualifiers FOREIGN key (qualifier_id) REFERENCES qualifiers (id) match simple
);

CREATE TABLE deletions(
       triple_id integer NOT NULL,
       qualifier_id integer NOT NULL,  -- gone here
       previous_qualifier_id integer NOT NULL CHECK (qualifier_id > previous_qualifier_id),  -- present here
       -- how to handle cases of open world where user never added triples themselves? zap at origin?
       -- have to zap based on the user's ranking at that point in time
       -- if you change the ranking then you can change the meaning of qualifiers
       CONSTRAINT fk__triple_qualifiers__qualifier_id__qualifiers FOREIGN key (qualifier_id) REFERENCES qualifiers (id) match simple,
       CONSTRAINT fk__triple_qualifiers__previous_qualifier_id__qualifiers FOREIGN key (qualifier_id) REFERENCES qualifiers (id) match simple
);

CREATE TABLE annotations(
       triple_id integer NOT NULL,  -- ah, and now we see the problem with having 3 tables
       p uri NOT NULL,
       o
);
